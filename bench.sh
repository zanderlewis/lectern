#!/bin/bash
set -e

# Lectern vs Composer Benchmark using Hyperfine
# This script uses hyperfine for accurate performance measurements with cache warm-up

REPORT_FILE="BENCHMARK.md"

echo "🔨 Building Lectern in release mode..."
cargo build --release --quiet

LECTERN="$(pwd)/target/release/lectern"
COMPOSER="composer"
BENCH_DIR="$(pwd)/bench-test"

echo ""
echo "=================================================="
echo "  Lectern vs Composer Performance Benchmark"
echo "=================================================="
echo ""

# Check if hyperfine is installed
if ! command -v hyperfine &> /dev/null; then
    echo "❌ hyperfine is not installed!"
    echo "Install it with: cargo install hyperfine"
    exit 1
fi

# Create a test project directory
echo "📦 Setting up test project..."
rm -rf "$BENCH_DIR"
mkdir -p "$BENCH_DIR"
cd "$BENCH_DIR"

# Create a simple composer.json
cat > composer.json << 'EOF'
{
    "name": "bench/test",
    "description": "Benchmark test project",
    "type": "project",
    "license": ["MIT"],
    "require": {
        "monolog/monolog": "^3.0",
        "guzzlehttp/guzzle": "^7.2"
    },
    "require-dev": {
        "phpunit/phpunit": "^10.0"
    },
    "autoload": {
        "psr-4": {
            "App\\": "src/"
        }
    },
    "minimum-stability": "stable",
    "prefer-stable": true
}
EOF

# Do initial install to populate vendor and lock files
echo "📥 Initial setup (installing packages)..."
if ! $LECTERN install > /dev/null 2>&1; then
    echo "❌ Failed to install packages with lectern"
    echo "Running with output for debugging:"
    $LECTERN install
    exit 1
fi

# Initialize markdown report
cd ..
cat > "$REPORT_FILE" << 'EOF'
# Lectern vs Composer Performance Benchmark

> Generated on: DATE_PLACEHOLDER

This benchmark compares the performance of Lectern against Composer across various common operations.

## System Information

- **Lectern Version**: `lectern --version` output
- **Composer Version**: `composer --version` output
- **Benchmark Tool**: hyperfine with 2 warmup runs and 5 test runs (3 for require/remove)

## Results Summary

| Command | Lectern | Composer | Speedup |
|---------|---------|----------|---------|
EOF

# Replace date placeholder
sed -i "s/DATE_PLACEHOLDER/$(date '+%Y-%m-%d %H:%M:%S')/" "$REPORT_FILE"

cd "$BENCH_DIR"

echo ""
echo "Starting benchmarks with warm caches..."
echo ""

# Install command
echo "📦 Benchmarking: install"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_install.md \
    --prepare "rm -rf vendor composer.lock" \
    --command-name "lectern install" "$LECTERN --quiet install" \
    --command-name "composer install" "$COMPOSER install --quiet --no-interaction"

# Update command
echo ""
echo "📦 Benchmarking: update"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_update.md \
    --command-name "lectern update" "$LECTERN --quiet update" \
    --command-name "composer update" "$COMPOSER update --quiet --no-interaction"

# Search command
echo ""
echo "🔍 Benchmarking: search"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_search.md \
    --command-name "lectern search" "$LECTERN --quiet search monolog" \
    --command-name "composer search" "$COMPOSER search monolog --quiet"

# Show command
echo ""
echo "📄 Benchmarking: show"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_show.md \
    --command-name "lectern show" "$LECTERN --quiet show monolog/monolog" \
    --command-name "composer show" "$COMPOSER show monolog/monolog --quiet"

# Outdated command
echo ""
echo "🔍 Benchmarking: outdated"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_outdated.md \
    --command-name "lectern outdated" "$LECTERN --quiet outdated" \
    --command-name "composer outdated" "$COMPOSER outdated --quiet"

# Licenses command
echo ""
echo "📜 Benchmarking: licenses"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_licenses.md \
    --command-name "lectern licenses" "$LECTERN --quiet licenses" \
    --command-name "composer licenses" "$COMPOSER licenses --quiet"

# Status command
echo ""
echo "📊 Benchmarking: status"
hyperfine --warmup 2 --runs 5 --export-markdown /tmp/bench_status.md \
    --command-name "lectern status" "$LECTERN --quiet status" \
    --command-name "composer show" "$COMPOSER show --quiet"

# Require command
echo ""
echo "➕ Benchmarking: require (add package)"
# Backup original files
cp composer.json composer.json.orig
cp composer.lock composer.lock.orig

hyperfine --warmup 1 --runs 3 --export-markdown /tmp/bench_require.md \
    --prepare "cp composer.json.orig composer.json; cp composer.lock.orig composer.lock" \
    --command-name "lectern require" "$LECTERN --quiet require symfony/yaml:^6.0" \
    --command-name "composer require" "$COMPOSER require symfony/yaml:^6.0 --quiet --no-interaction"

# Remove command - first add the package to both so we have a consistent starting state
echo ""
echo "➖ Benchmarking: remove (remove package)"
# Create versions with symfony/yaml added by each tool
cp composer.json.orig composer.json
cp composer.lock.orig composer.lock
$LECTERN --quiet require symfony/yaml:^6.0 > /dev/null 2>&1
cp composer.json composer.json.lectern
cp composer.lock composer.lock.lectern

cp composer.json.orig composer.json
cp composer.lock.orig composer.lock
$COMPOSER require symfony/yaml:^6.0 --quiet --no-interaction > /dev/null 2>&1
cp composer.json composer.json.composer
cp composer.lock composer.lock.composer

hyperfine --warmup 1 --runs 3 --export-markdown /tmp/bench_remove.md \
    --prepare "cp composer.json.lectern composer.json; cp composer.lock.lectern composer.lock" \
    --command-name "lectern remove" "$LECTERN --quiet remove symfony/yaml" \
    --prepare "cp composer.json.composer composer.json; cp composer.lock.composer composer.lock" \
    --command-name "composer remove" "$COMPOSER remove symfony/yaml --quiet --no-interaction"

# Cleanup
cd ..
rm -rf "$BENCH_DIR"

# Generate the markdown report
echo ""
echo "📊 Generating report..."

# Function to extract mean time from hyperfine markdown output
get_mean_time() {
    local file=$1
    local command=$2
    # Extract the mean time from the markdown table (column 2, which is the "Mean" column)
    # The format is: | `command name` | mean ± std | min | max | relative |
    grep "\`$command" "$file" | awk -F'|' '{print $3}' | awk '{print $1}'
}

# Function to calculate speedup
calc_speedup() {
    local lectern=$1
    local composer=$2
    if command -v bc &> /dev/null; then
        echo "scale=1; $composer / $lectern" | bc
    else
        awk "BEGIN {printf \"%.1f\", $composer / $lectern}"
    fi
}

# Create report header
cat > "$REPORT_FILE" << EOF
# Lectern vs Composer Performance Benchmark

> Generated on: $(date '+%Y-%m-%d %H:%M:%S')

This benchmark compares the performance of Lectern against Composer across various common operations.

## System Information

- **Benchmark Tool**: hyperfine with 2 warmup runs and 5 test runs (3 for require/remove)
- **Test Setup**: Warm cache conditions for both tools

## Results Summary

| Command | Lectern (ms) | Composer (ms) | Speedup |
|---------|--------------|---------------|---------|
EOF

# Process each benchmark and add to summary table
for bench in install update search show outdated licenses status require remove; do
    local_file="/tmp/bench_${bench}.md"
    if [ -f "$local_file" ]; then
        lectern_time=$(get_mean_time "$local_file" "lectern")
        composer_time=$(get_mean_time "$local_file" "composer")
        
        if [ -n "$lectern_time" ] && [ -n "$composer_time" ]; then
            # Remove any non-numeric characters except dots
            lectern_clean=$(echo "$lectern_time" | tr -cd '0-9.')
            composer_clean=$(echo "$composer_time" | tr -cd '0-9.')
            
            if [ -n "$lectern_clean" ] && [ -n "$composer_clean" ]; then
                speedup=$(calc_speedup "$lectern_clean" "$composer_clean")
                echo "| $bench | $lectern_clean | $composer_clean | ${speedup}x |" >> "$REPORT_FILE"
            fi
        fi
    fi
done

# Add detailed results section with full hyperfine tables
cat >> "$REPORT_FILE" << 'EOF'

## Detailed Results

Each benchmark includes mean execution time ± standard deviation, with min and max values.

### Install Command

Fresh installation of all dependencies.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_install.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Update Command

Update all dependencies to their latest allowed versions.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_update.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Search Command

Search for packages on Packagist.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_search.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Show Command

Display detailed information about a specific package.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_show.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Outdated Command

Check for outdated packages.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_outdated.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Licenses Command

Display licenses of installed packages.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_licenses.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Status Command

Show status of installed packages.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_status.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Require Command

Add a new package to the project.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_require.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

### Remove Command

Remove a package from the project.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
EOF
tail -n +2 /tmp/bench_remove.md >> "$REPORT_FILE"

cat >> "$REPORT_FILE" << 'EOF'

---

## Notes

- All benchmarks are run with warm caches to measure steady-state performance
- Times shown are in milliseconds (ms)
- Speedup is calculated as: Composer time / Lectern time
- Each command is run multiple times (2 warmup + 5 test runs, or 1 warmup + 3 test runs for require/remove)

EOF

# Cleanup temp files
rm -f /tmp/bench_*.md

echo ""
echo "=================================================="
echo "  ✅ Benchmark Complete!"
echo "=================================================="
echo "📄 Report saved to: $REPORT_FILE"
echo ""

